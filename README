BEGIN
=====
A benchmark dataset for evaluating dialog system and natural language generation metrics.
See our paper "Evaluating Groundedness in Dialogue Systems: The BEGIN Benchmark".


Authors
-------
* Nouha Dziri
* Hannah Rashkin <hrashkin@google.com>
* Tal Linzen
* David Reitter <reitter@google.com>


Versions
--------
* Example release 4/2021: Only examples included


Format
------

The format of the TSV file is:

	*evidence:  The selected document span (coming from Wizard of Wikipedia (Dinan et. al 2019))
	*previous turn:  The previous utterance in the conversation  (coming from Wizard of Wikipedia (Dinan et. al 2019))	
	*response:  A response generated by a large LM (either GPT2 or T5)
	*gold label: An aggregated label from human annotators which is either: entailment, contradiction, hallucination, generic, or off-topic
	*full label set: The set of annotations from humans as a ";" delimited segment with annotations for whether the response is:
		 **"generic"/"not-generic" - whether it was labelled as generic or not, 
		 **"off-topic"/"on-topic" - whether it was labelled as off-topic or not, 
		 **"cooperative"/"uncooperative" - whether it was labelled as cooperative or not, 
		 **"not-info/faithful/contradiction/hallucination-personal/hallucination-info" - whether it was intended to be informative and how faithful and supported the information is with respect to the evidence. We distinguish between "not-info" (examples that are not marked specifically as intending to be informative), "hallucination-personal" (examples that are marked as containing personal/subjective experiences or opinions) and "hallucination-info" (unsupported information)